{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPVCW/1EoEbxm8e2Ln2d++D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mrXXJ9ERnqwH"},"outputs":[],"source":["#.head() returns the first few rows (the “head” of the DataFrame).\n",".info() shows information on each of the columns, such as the data type and number of missing values.\n",".shape returns the number of rows and columns of the DataFrame.\n",".describe() calculates a few summary statistics for each column.\n","\n","#importing pandas using the alias pd\n","#import pandas as pd\n","\n"]},{"cell_type":"markdown","source":["Sorting & Subsetting"],"metadata":{"id":"VOBVuoq2u3sx"}},{"cell_type":"code","source":["# Sort homelessness by descending family members\n","homelessness_fam = homelessness.sort_values(\"family_members\", ascending=False)\n","\n","# Print the top few rows\n","print(homelessness_fam.head())\n","\n","# Sort homelessness by region, then descending family members\n","homelessness_reg_fam = homelessness.sort_values(['region','family_members'],ascending=[True,False])\n","\n","# Print the top few rows\n","print(homelessness_reg_fam.head())\n","\n","# Select the individuals column\n","individuals = homelessness['individuals']\n","\n","print(individuals.head())\n","\n","# Select the state and family_members columns\n","state_fam = homelessness[['state','family_members']]\n","                                      #Note the double bracket vs the single bracket in previous example\n","print(state_fam.head())\n","\n","# Select only the individuals and state columns, in that order\n","ind_state = homelessness[[\"individuals\", \"state\"]]\n","\n","print(ind_state.head())\n","\n","# Filter for rows where individuals is greater than 10000\n","ind_gt_10k = homelessness[homelessness['individuals'] > 10000]\n","                                          #why use homelessness twice in this example?\n","# See the result\n","print(ind_gt_10k)\n","                   region       state  individuals  family_members  state_pop\n","    4              Pacific  California     109008.0         20964.0   39461588\n","    9       South Atlantic     Florida      21443.0          9587.0   21244317\n","    32        Mid-Atlantic    New York      39827.0         52070.0   19530351\n","    37             Pacific      Oregon      11139.0          3337.0    4181886\n","    43  West South Central       Texas      19199.0          6111.0   28628666\n","    47             Pacific  Washington      16424.0          5880.0    7523869\n","\n","# Filter for rows where region is Mountain\n","mountain_reg = homelessness[homelessness['region']=='Mountain']\n","                                #Note double bracket and double equal sign? Also, homelessness twice\n","\n","# See the result\n","print(mountain_reg)\n","\n","         region       state  individuals  family_members  state_pop\n","    2   Mountain     Arizona       7259.0          2606.0    7158024\n","    5   Mountain    Colorado       7607.0          3250.0    5691287\n","    12  Mountain       Idaho       1297.0           715.0    1750536\n","    26  Mountain     Montana        983.0           422.0    1060665\n","    28  Mountain      Nevada       7058.0           486.0    3027341\n","    31  Mountain  New Mexico       1949.0           602.0    2092741\n","    44  Mountain        Utah       1904.0           972.0    3153550\n","    50  Mountain     Wyoming        434.0           205.0     577601\n","# Filter for rows where family_members is less than 1000\n","# and region is Pacific\n","fam_lt_1k_pac = homelessness[(homelessness[\"family_members\"] < 1000) & (homelessness[\"region\"] == \"Pacific\")]\n","\n","# See the result\n","print(fam_lt_1k_pac)\n","\n","\n","# The Mojave Desert states\n","canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n","\n","# Filter for rows in the Mojave Desert states\n","mojave_homelessness = homelessness[homelessness[\"state\"].isin(canu)]\n","# why canu not in parentheses ****\n","# See the result\n","print(mojave_homelessness)\n","\n","# Add total col as sum of individuals and family_members\n","homelessness['total']=homelessness['individuals']+homelessness['family_members']\n","\n","# Add p_homeless col as proportion of total homeless population to the state population\n","homelessness['p_homeless']=homelessness['total']/homelessness['state_pop']\n","\n","# See the result\n","print(homelessness)\n","\n","# Create indiv_per_10k col as homeless individuals per 10k state pop\n","homelessness[\"indiv_per_10k\"] = 10000 * homelessness['individuals'] / homelessness['state_pop']\n","\n","# Subset rows for indiv_per_10k greater than 20\n","high_homelessness = homelessness[homelessness[\"indiv_per_10k\"] > 20]\n","\n","# Sort high_homelessness by descending indiv_per_10k\n","high_homelessness_srt = high_homelessness.sort_values(\"indiv_per_10k\",ascending=False)\n","                                      #why the round brackets in this example vs the square in the previous\n","\n","\n","# From high_homelessness_srt, select the state and indiv_per_10k cols\n","result = high_homelessness_srt[['state','indiv_per_10k']]\n","\n","# See the result\n","print(result)\n","\n","# Print the head of the sales DataFrame\n","print(sales.head())\n","\n","# Print the info about the sales DataFrame\n","print(sales.info())\n","\n","# Print the mean of weekly_sales\n","print(sales['weekly_sales'].mean())\n","\n","# Print the median of weekly_sales\n","print(sales['weekly_sales'].median())\n","\n","# Print the maximum of the date column\n","print(sales['date'].max())\n","\n","# Print the minimum of the date column\n","print(sales['date'].min())\n","\n","# A custom IQR function\n","def iqr(column):\n","    return column.quantile(0.75) - column.quantile(0.25)\n","\n","# Print IQR of the temperature_c column\n","print(sales['temperature_c'].agg(iqr))\n","\n","# Import NumPy and create custom IQR function\n","import numpy as np\n","def iqr(column):\n","    return column.quantile(0.75) - column.quantile(0.25)\n","\n","# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n","print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))\n","\n","# Sort sales_1_1 by date\n","sales_1_1 = sales_1_1.sort_values(\"date\")\n","\n","# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\n","sales_1_1[\"cum_weekly_sales\"] = sales_1_1[\"weekly_sales\"].cumsum()\n","\n","# Get the cumulative max of weekly_sales, add as cum_max_sales col\n","sales_1_1[\"cum_max_sales\"] = sales_1_1[\"weekly_sales\"].cummax()\n","\n","# See the columns you calculated\n","print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])\n","\n","# Drop duplicate store/type combinations\n","store_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\n","print(store_types.head())\n","\n","# Drop duplicate store/department combinations\n","store_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\n","print(store_depts.head())\n","\n","# Subset the rows where is_holiday is True and drop duplicate dates\n","holiday_dates = sales[sales[\"is_holiday\"]].drop_duplicates(subset=\"date\")\n","\n","# Print date col of holiday_dates\n","print(holiday_dates[\"date\"])\n","\n","# Count the number of stores of each type\n","store_counts = store_types[\"type\"].value_counts()\n","print(store_counts)\n","\n","# Get the proportion of stores of each type\n","store_props = store_types[\"type\"].value_counts(normalize=True)\n","print(store_props)\n","\n","# Count the number of stores of each department and sort\n","dept_counts_sorted = store_depts[\"department\"].value_counts(sort=True)\n","print(dept_counts_sorted)\n","\n","# Get the proportion of stores of each department and sort\n","dept_props_sorted = store_depts[\"department\"].value_counts(sort=True, normalize=True)\n","print(dept_props_sorted)\n","\n","# Calc total weekly sales\n","sales_all = sales[\"weekly_sales\"].sum()\n","\n","# Subset for type A stores, calc total weekly sales\n","sales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n","\n","# Subset for type B stores, calc total weekly sales\n","sales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n","\n","# Subset for type C stores, calc total weekly sales\n","sales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n","\n","# Get proportion for each type\n","sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\n","print(sales_propn_by_type)\n","\n","#The above example is much easier to do with the following methods.\n","\n","# Group by type; calc total weekly sales\n","sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n","\n","# Get proportion for each type\n","sales_propn_by_type = sales_by_type / sum(sales_by_type)\n","print(sales_propn_by_type)\n","\n","\n","# From previous step\n","sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n","\n","# Group by type and is_holiday; calc total weekly sales\n","sales_by_type_is_holiday = sales.groupby(['type','is_holiday'])['weekly_sales'].sum()\n","print(sales_by_type_is_holiday)\n","\n","# Import numpy with the alias np\n","import numpy as np\n","\n","# For each store type, aggregate weekly_sales: get min, max, mean, and median\n","sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n","\n","# Print sales_stats\n","print(sales_stats)\n","\n","# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n","unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n","\n","# Print unemp_fuel_stats\n","print(unemp_fuel_stats)"],"metadata":{"id":"WaZFD8MEu90k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pivot Tables in Data Manipulation with Pandas\n"],"metadata":{"id":"F3T0vnFr6kbk"}},{"cell_type":"code","source":["\n","# Pivot for mean weekly_sales for each store type\n","mean_sales_by_type = sales.pivot_table(values='weekly_sales',index='type')\n","\n","# Print mean_sales_by_type\n","print(mean_sales_by_type)\n","\n","# Import NumPy as np\n","import numpy as np\n","\n","# Pivot for mean and median weekly_sales for each store type\n","mean_med_sales_by_type = sales.pivot_table(values='weekly_sales', index='type', aggfunc=[np.mean, np.median])\n","\n","# Print mean_med_sales_by_type\n","print(mean_med_sales_by_type)\n","\n","# Pivot for mean weekly_sales by store type and holiday\n","mean_sales_by_type_holiday = sales.pivot_table(values='weekly_sales',index='type', columns='is_holiday')\n","\n","# Print mean_sales_by_type_holiday\n","print(mean_sales_by_type_holiday)\n","\n","# Print mean weekly_sales by department and type; fill missing values with 0\n","print(sales.pivot_table(values='weekly_sales',index='department', columns='type', fill_value=0))\n","\n","# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n","print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0, margins=True))\n","\n"],"metadata":{"id":"uKbZUfiH6nAo"},"execution_count":null,"outputs":[]}]}