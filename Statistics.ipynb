{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMpB+0BkFZvuiuHKMJ18zI9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Gsv6TlhI8BYK"},"outputs":[],"source":["#Import numpy with the alias np.\n","#Create DataFrame be_consumption that holds the ROWS of food_consumption for 'Belgium'.\n","#Create another DataFrame usa_consumption that holds the ROWS for 'USA'.\n","#Calculate the mean and median of food consumptionfor both countries.\n","\n","# Import numpy with alias np\n","import numpy as np\n","\n","# Filter for Belgium\n","be_consumption = food_consumption[food_consumption['country'] == 'Belgium']\n","\n","# Filter for USA\n","usa_consumption = food_consumption[food_consumption['country'] == 'USA']\n","\n","# Calculate mean and median consumption in Belgium\n","print(np.mean(be_consumption['consumption']))\n","print(np.median(be_consumption['consumption']))\n","\n","# Calculate mean and median consumption in USA\n","print(np.mean(usa_consumption['consumption']))\n","print(np.median(usa_consumption['consumption']))\n","\n","# Import matplotlib.pyplot with alias plt\n","import matplotlib.pyplot as plt\n","\n","# Subset for food_category equals rice\n","rice_consumption = food_consumption[food_consumption['food_category']=='rice']\n","\n","# Histogram of co2_emission for rice and show plot\n","rice_consumption['co2_emission'].hist()\n","plt.show()\n","\n","\n","# Calculate mean and median of co2_emission with .agg()\n","print(rice_consumption['co2_emission'].agg([np.mean, np.median]))\n","-----------------------------------------------------------------------------\n","#Quartile and quintile are types of quantile.\n","\n","# Calculate the quartiles of co2_emission\n","print(np.quantile(food_consumption['co2_emission'] , [0, 0.25, 0.5, 0.75,1]))\n","\n","# Calculate the quintiles of co2_emission\n","print(np.quantile(food_consumption['co2_emission'],[0,.2,.4,.6,.8,1]))\n","\n","# Calculate the deciles of co2_emission\n","print(np.quantile(food_consumption['co2_emission'], [0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1]))\n","\n","\n","# Print variance and sd of co2_emission for each food_category\n","print(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))\n","\n","# Import matplotlib.pyplot with alias plt\n","import matplotlib.pyplot as plt\n","\n","# Create histogram of co2_emission for food_category 'beef'\n","food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist()\n","# Show plot\n","plt.show()\n","\n","# Create histogram of co2_emission for food_category 'eggs'\n","food_consumption[food_consumption['food_category'] == 'eggs']['co2_emission'].hist()\n","# Show plot\n","plt.show()\n","\n","# Calculate total co2_emission per country: emissions_by_country\n","emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n","\n","print(emissions_by_country)\n","\n","# Compute the first and third quartiles and IQR of emissions_by_country\n","q1 = np.quantile(emissions_by_country, 0.25)\n","q3 = np.quantile(emissions_by_country, 0.75)\n","iqr = q3 - q1\n","\n","# Calculate the lower and upper cutoffs for outliers\n","lower = q1 - 1.5 * iqr\n","upper = q3 + 1.5 * iqr\n","\n","#Subset emissions_by_country to get countries with a total emission greater than\n","#the upper cutoff or a total emission less than the lower cutoff.\n","# Subset emissions_by_country to find outliers\n","outliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)]\n","print(outliers)\n","------------------------------------------------------------------------------------------\n","\n","# P(event)= Number ways an event can happen/total Number of possible outcomes\n","#Count the number of deals Amir worked on for each product type using .value_counts()\n","#and store in counts.\n","\n","# Count the deals for each product\n","counts = amir_deals['product'].value_counts()\n","print(counts)\n","\n","\n","#Calculate the probability of selecting a deal for the different product types by dividing the counts\n","#by the total number of deals Amir worked on. Save this as probs.\n","#The total number of deals Amir worked on is equal to the number of rows in the DataFrame.\n","\n","# Calculate probability of picking a deal with each product\n","probs =counts/178\n","print(probs)\n","\n","# Set random seed\n","np.random.seed(24)\n","\n","# Sample 5 deals without replacement\n","sample_without_replacement = amir_deals.sample(5, replace=False)\n","print(sample_without_replacement)\n","\n","# Sample 5 deals with replacement\n","sample_with_replacement = amir_deals.sample(5, replacement=True)\n","print(sample_with_replacement)\n"]},{"cell_type":"markdown","source":["Discrete Distributions"],"metadata":{"id":"OIna9ZsTcFZd"}},{"cell_type":"code","source":["#Create a histogram of the group_size column of restaurant_groups,\n","#setting bins to [2, 3, 4, 5, 6]. Remember to show the plot.\n","\n","# Create a histogram of restaurant_groups and show plot\n","restaurant_groups['group_size'].hist(bins=[2,3,4,5,6])\n","plt.show()\n","\n","#Count the number of each group_size in restaurant_groups, then divide by the\n","#number of rows in restaurant_groups to calculate the probability of randomly selecting\n","#a group of each size. Save as size_dist.\n","#Reset the index of size_dist.\n","#Rename the columns of size_dist to group_size and prob.\n","#.value_counts() must be applied to a Series.\n","#You can get the number of rows in a DataFrame using df.shape[0].\n","\n","# Create probability distribution\n","size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n","\n","# Reset index and rename columns\n","size_dist = size_dist.reset_index()\n","size_dist.columns = ['group_size', 'prob']\n","\n","#Calculate the expected value of the size_dist, which represents the expected group size,\n","#by multiplying the group_size by the prob and taking the sum.\n","# Calculate expected value\n","\n","expected_value = np.sum(size_dist.group_size*size_dist.prob)\n","print(expected_value)\n","\n","\n","#Yes, the expression size_dist.group_size * size_dist.prob is equivalent to\n","#size_dist['group_size'] * size_dist['prob'] in Pandas, as long as group_size and prob\n","#are valid column names in the DataFrame size_dist\n","\n","# Subset groups of size 4 or more\n","groups_4_or_more = size_dist[size_dist['group_size'] >= 4]\n","\n","# Sum the probabilities of groups_4_or_more\n","prob_4_or_more = np.sum(groups_4_or_more['prob'])\n","print(prob_4_or_more)\n","-------------------------------------------------------------------\n","\n","# Min and max wait times for back-up that happens every 30 min\n","min_time = 0\n","max_time = 30\n","\n","# Import uniform from scipy.stats\n","from scipy.stats import uniform\n","\n","# Calculate probability of waiting less than 5 mins\n","prob_less_than_5 = uniform.cdf(5,0,30)\n","print(prob_less_than_5)\n","\n","# Calculate probability of waiting more than 5 mins\n","prob_greater_than_5 = 1 - uniform.cdf(5,0,30)\n","print(prob_greater_than_5)\n","\n","# Calculate probability of waiting 10-20 mins\n","prob_between_10_and_20 =uniform.cdf(20,0,30)-uniform.cdf(10,0,30)\n","\n","# Set random seed to 334\n","np.random.seed(334)\n","\n","# Import uniform\n","from scipy.stats import uniform\n","\n","# Generate 1000 wait times between 0 and 30 mins\n","wait_times = uniform.rvs(0, 30, size=1000)\n","\n","# Create a histogram of simulated times and show plot\n","plt.hist(wait_times)\n","plt.show()\n","\n","\n","#Simulate 1 deal worked on by Amir, who wins 30% of the deals he works on.\n","\n","# Import binom from scipy.stats\n","from scipy.stats import binom\n","\n","# Set random seed to 10\n","np.random.seed(10)\n","\n","# Simulate a single deal\n","print(binom.rvs(1, .3, size=1))\n","\n","# Simulate 1 week of 3 deals\n","print(binom.rvs(3,.3,size=1))\n","\n","# Simulate 52 weeks of 3 deals\n","deals = binom.rvs(3, 0.3, size=52)\n","\n","# Print mean deals won per week\n","print(np.mean(deals))\n","\n","\n","# binom.pmf(num heads, num trials, prob of heads)\n","# Probability of closing 3 out of 3 deals\n","prob_3 = binom.pmf(3, 3, 0.3)\n","\n","#To calculate the probability of closing less than or equal to 1 deal,\n","#pass 1 as the first argument to binom.cdf()\n","\n","# Probability of closing <= 1 deal out of 3 deals\n","prob_less_than_or_equal_1 = binom.cdf(1, 3, 0.3)\n","\n","# Probability of closing > 1 deal out of 3 deals\n","prob_greater_than_1 = 1 - binom.cdf(1, 3, 0.3)\n","\n","\n","#Calculate the expected number of sales out of the 3 he works on that Amir will win each week if he maintains his 30% win rate.\n","#Calculate the expected number of sales out of the 3 he works on that he'll win if his win rate drops to 25%.\n","#Calculate the expected number of sales out of the 3 he works on that he'll win if his win rate rises to 35%.\n","# Expected number won with 30% win rate\n","won_30pct = 3 * 0.3\n","print(won_30pct)\n","\n","# Expected number won with 25% win rate\n","won_25pct = 3 * 0.25\n","print(won_25pct)\n","\n","# Expected number won with 35% win rate\n","won_35pct = 3 * 0.35\n","print(won_35pct)\n","-------------------------------------------------------------------------"],"metadata":{"id":"lBvNh7s9cI8j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Normal Distribution"],"metadata":{"id":"phbkAfFWDB1c"}},{"cell_type":"code","source":["# Histogram of amount with 10 bins and show plot\n","amir_deals['amount'].hist(bins=10)\n","plt.show()\n","\n","#norm.cdf( ,mean, standard deviation)\n","# Probability of deal < 7500\n","prob_less_7500 = norm.cdf(7500, 5000, 2000)\n","\n","#The probability of making a sale over x is the same as\n","# 1- the probability of making a sale less than\n","\n","# Probability of deal > 1000\n","prob_over_1000 = 1 - norm.cdf(1000, 5000, 2000)\n","\n","# Probability of deal between 3000 and 7000\n","prob_3000_to_7000 = norm.cdf(7000,5000,2000)-norm.cdf(3000,5000,2000)\n","\n","# Calculate amount that 25% of deals will be less than\n","pct_25 = norm.ppf(0.25, 5000, 2000)\n","\n","\n","#Currently, Amir's average sale amount is $5000. Calculate what his new average amount\n","#will be if it increases by 20% and store this in new_mean.\n","# Calculate new average amount\n","new_mean = 5000 * 1.2\n","\n","#Amir's current standard deviation is $2000. Calculate what his new standard deviation\n","#will be if it increases by 30% and store this in new_sd.\n","# Calculate new standard deviation\n","new_sd = 2000 * 1.3\n","\n","#Create a variable called new_sales, which contains 36 simulated amounts from a normal distribution\n","#with a mean of new_mean and a standard deviation of new_sd.\n","\n","# Simulate 36 new sales\n","new_sales = norm.rvs(new_mean, new_sd, size=36)\n","\n","#Plot the distribution of the new_sales amounts using a histogram and show the plot.\n","# Create histogram and show\n","plt.hist(new_sales)\n","plt.show()\n"],"metadata":{"id":"ci_5BIGaDGbE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The Central Limit Theorem"],"metadata":{"id":"4wLuz6DXOGkk"}},{"cell_type":"code","source":["die = pd.Series([1, 2, 3, 4, 5, 6])\n","# Roll 5 times\n","samp_5 = die.sample(5, replace=True)\n","print(samp_5)\n","\n","array([3, 1, 4, 1, 1])\n","\n","np.mean(samp_5)\n","\n","2.0\n","\n","# Create a histogram of num_users and show\n","amir_deals['num_users'].hist()\n","plt.show()\n","\n","# Set seed to 104\n","np.random.seed(104)\n","\n","# Sample 20 num_users with replacement from amir_deals\n","samp_20 = amir_deals['num_users'].sample(20, replace=True)\n","\n","# Take mean of samp_20\n","print(np.mean(samp_20))\n","\n","# Loop 100 times\n","for i in range(100):\n","  # Take sample of 20 num_users\n","  samp_20 = amir_deals['num_users'].sample(20, replace=True)\n","  # Calculate mean of samp_20\n","  samp_20_mean = np.mean(samp_20)\n","  # Append samp_20_mean to sample_means\n","  sample_means.append(samp_20_mean)\n","\n","print(sample_means)\n","\n","# Convert to Series and plot histogram\n","sample_means_series = pd.Series(sample_means)\n","sample_means_series.hist()\n","# Show plot\n","plt.show()\n","\n","# Set seed to 321\n","np.random.seed(321)\n","\n","sample_means = []\n","# Loop 30 times to take 30 means\n","for i in range(30):\n","  # Take sample of size 20 from num_users col of all_deals with replacement\n","  cur_sample = all_deals['num_users'].sample(20, replace=True)\n","  # Take mean of cur_sample\n","  cur_mean = np.mean(cur_sample)\n","  # Append cur_mean to sample_means\n","  sample_means.append(cur_mean)\n","\n","# Print mean of sample_means\n","print(np.mean(sample_means))\n","\n","# Print mean of num_users in amir_deals\n","print(np.mean(amir_deals['num_users']))\n"],"metadata":{"id":"Jdkdl-WAOUCu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The Poisson Distribution-Probability of some # of events occuring over a fixed period of time.\n","Ex:\n","Probability of 12 people arriving at a restaurant per hour.\n","Probability of < 20 earthquakes per year"],"metadata":{"id":"e_XwGYDthCnG"}},{"cell_type":"code","source":["# Import poisson from scipy.stats\n","from scipy.stats import poisson\n","\n","#Import poisson from scipy.stats and calculate the probability that Amir responds\n","#to 5 leads in a day, given that he responds to an average of 4.\n","\n","# Probability of 5 responses\n","prob_5 = poisson.pmf(5,4)\n","\n","print(prob_5)\n","\n","#Amir's coworker responds to an average of 5.5 leads per day.\n","#What is the probability that she answers 5 leads in a day?\n","# Probability of 5 responses\n","prob_coworker = poisson.pmf(5,5.5)\n","\n","#What's the probability that Amir responds to 2 or fewer leads in a day?\n","# Probability of 2 or fewer responses\n","prob_2_or_less = poisson.cdf(2,4)\n","\n","#What's the probability that Amir responds to more than 10 leads in a day?\n","# Probability of > 10 responses\n","\n","prob_over_10 = 1-poisson.cdf(10,4)\n","\n","#Notice the 1- and also where 10 needs to be in the equation.\n","\n"],"metadata":{"id":"bUpj_tqShGd8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exponential Distribution: Probability of time between Poisson events.\n","Probability of >1 day between adoptions\n","Probability of < 10 minutes between restaurant arrivals."],"metadata":{"id":"duMjnyqQkPYE"}},{"cell_type":"code","source":["# Import expon from scipy.stats\n","from scipy.stats import expon\n","\n","# Print probability response takes < 1 hour\n","print(expon.cdf(1, scale=2.5))\n","\n","# Print probability response takes > 4 hours\n","print(1-expon.cdf(4, scale=2.5))\n","\n","# Print probability response takes 3-4 hours\n","\n","print(expon.cdf(4, scale=2.5)-expon.cdf(3,scale=2.5))\n","\n"],"metadata":{"id":"KowUGoq9kU4S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Correlation"],"metadata":{"id":"a1D10PCQSJOy"}},{"cell_type":"code","source":["# Create a scatterplot of happiness_score vs. life_exp and show\n","sns.scatterplot(x='life_exp', y='happiness_score', data=world_happiness)\n","\n","# Show plot\n","plt.show()\n","\n","# Create scatterplot of happiness_score vs life_exp with trendline\n","sns.lmplot(x=\"life_exp\", y=\"happiness_score\", data=world_happiness, ci=None)\n","\n","# Show plot\n","plt.show()\n","\n","#Does NOT matter which goes first for correlation, life_exp or happiness_score.\n","# Correlation between life_exp and happiness_score\n","cor = world_happiness['life_exp'].corr(world_happiness['happiness_score'])\n","\n","# Scatterplot of gdp_per_cap and life_exp\n","sns.scatterplot(x='gdp_per_cap', y='life_exp', data=world_happiness)\n","\n","# Correlation between gdp_per_cap and life_exp\n","cor = world_happiness['gdp_per_cap'].corr(world_happiness['life_exp'])\n","\n","# Scatterplot of happiness_score vs. gdp_per_cap\n","sns.scatterplot(x='gdp_per_cap', y='happiness_score', data=world_happiness)\n","plt.show()\n","\n","# Calculate correlation\n","cor = world_happiness['gdp_per_cap'].corr(world_happiness['happiness_score'])\n","print(cor)\n","\n","# Create log_gdp_per_cap column\n","world_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'])\n","\n","# Scatterplot of happiness_score vs. log_gdp_per_cap\n","sns.scatterplot(x='log_gdp_per_cap', y='happiness_score', data=world_happiness)\n","plt.show()\n","\n","# Calculate correlation\n","cor = world_happiness['log_gdp_per_cap'].corr(world_happiness['happiness_score'])\n","print(cor)\n","\n"],"metadata":{"id":"qBqOCkY0SK5I"},"execution_count":null,"outputs":[]}]}