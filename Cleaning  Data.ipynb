{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHdPQNzYg8j2ziu8PcB21Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"nXkhBrxuOOrd"},"outputs":[],"source":["Print the information of ride_sharing.\n","Use .describe() to print the summary statistics of the user_type column from ride_sharing."]},{"cell_type":"code","source":["# Print the information of ride_sharing\n","print(ride_sharing.info())\n","\n","# Print summary statistics of user_type column\n","print(ride_sharing['user_type'].describe())"],"metadata":{"id":"FS3Mfxn0OXFD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the information of ride_sharing\n","print(ride_sharing.info())\n","\n","# Print summary statistics of user_type column\n","print(ride_sharing['user_type'].describe())\n","\n","# Convert user_type from integer to category\n","ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n","\n","# Write an assert statement confirming the change\n","assert ride_sharing['user_type_cat'].dtype == 'category'\n","\n","# Print new summary statistics\n","print(ride_sharing['user_type_cat'].describe())\n","\n","# Strip duration of minutes\n","ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n","\n","# Convert duration to integer\n","ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n","\n","# Write an assert statement making sure of conversion\n","assert ride_sharing['duration_time'].dtype == 'int'\n","\n","# Print formed columns and calculate average ride duration\n","print(ride_sharing[['duration','duration_trim','duration_time']])\n","print(ride_sharing['duration_time'].mean())\n"],"metadata":{"id":"BHQ_M5ycpJhT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Data Range Constraints"],"metadata":{"id":"_-JUF3XaqQpU"}},{"cell_type":"code","source":["# Convert tire_sizes to integer\n","ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n","\n","# Set all values above 27 to 27\n","ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n","\n","# Reconvert tire_sizes back to categorical\n","ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n","\n","# Print tire size description\n","print(ride_sharing['tire_sizes'].describe())\n","\n","# Convert ride_date to date\n","ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n","\n","# Save today's date\n","today = dt.date.today()\n","\n","# Set all in the future to today's date\n","ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n","\n","# Print maximum of ride_dt column\n","print(ride_sharing['ride_dt'].max())\n"],"metadata":{"id":"LMYAI0r_qTaY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Uniqueness Constraints"],"metadata":{"id":"313IdL8jt69n"}},{"cell_type":"markdown","source":["Find duplicated rows of ride_id in the ride_sharing DataFrame while setting keep to False.\n","Subset ride_sharing on duplicates and sort by ride_id and assign the results to duplicated_rides.\n","Print the ride_id, duration and user_birth_year columns of duplicated_rides in that order."],"metadata":{"id":"jTk7LL92wUgV"}},{"cell_type":"code","source":["# Find duplicates\n","duplicates = ride_sharing.duplicated(subset = 'ride_id', keep = False)\n","\n","# Sort your duplicated rides\n","duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n","\n","# Print relevant columns\n","print(duplicated_rides[['ride_id','duration','user_birth_year']])"],"metadata":{"id":"Gj7KjzfQt98S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop complete duplicates from ride_sharing\n","ride_dup = ride_sharing.drop_duplicates()\n","\n","# Create statistics dictionary for aggregation function\n","statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n","\n","# Group by ride_id and compute new statistics\n","ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n","\n","# Find duplicated values again\n","duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n","duplicated_rides = ride_unique[duplicates == True]\n","\n","# Assert duplicates are processed\n","assert duplicated_rides.shape[0] == 0"],"metadata":{"id":"DqRLg3Q6gJHx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Membership Constraints"],"metadata":{"id":"KvFcjc2JgXYT"}},{"cell_type":"code","source":["# Print categories DataFrame\n","print(categories)\n","\n","# Print unique values of survey columns in airlines\n","print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n","print('Safety: ', airlines['safety'].unique(), \"\\n\")\n","print('Satisfaction: ', airlines['satisfaction'].unique(),\"\\n\")\n"],"metadata":{"id":"jj_Fnd7gDgf9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create a set out of the cleanliness column in airlines using set() and find the inconsistent category by finding the difference in the cleanliness column of categories.\n","Find rows of airlines with a cleanliness value not in categories and print the output."],"metadata":{"id":"xzhqMLHnF3ZY"}},{"cell_type":"code","source":["# Find the cleanliness category in airlines not in categories\n","cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n","\n","# Find rows with that category\n","cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n","\n","# Print rows with inconsistent category\n","print(airlines[cat_clean_rows])\n","\n","# Print rows with consistent categories only\n","#This is really close to printing rows with the inconsistent category from the\n","#last step, you just need to add one small symbol while subsetting.\n","print(airlines[~cat_clean_rows])"],"metadata":{"id":"QDkT4BohFzh4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Categorical Variables"],"metadata":{"id":"AMjEX_G3_U-0"}},{"cell_type":"code","source":["# Print unique values of both columns\n","print(airlines['dest_region'].unique())\n","print(airlines['dest_size'].unique())\n","\n","# Print unique values of both columns\n","print(airlines['dest_region'].unique())\n","print(airlines['dest_size'].unique())\n","\n","# Lower dest_region column and then replace \"eur\" with \"europe\"\n","airlines['dest_region'] = airlines['dest_region'].str.lower()\n","airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n","\n","# Remove white spaces from `dest_size`\n","airlines['dest_size'] = airlines['dest_size'].str.strip()\n","\n","# Verify changes have been effected\n","print(airlines['dest_size'].unique())\n","print(airlines['dest_region'].unique())\n"],"metadata":{"id":"tzRvPhYKB3_j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The airlines DataFrame contains the day and wait_min columns, which are categorical and numerical respectively. The day column contains the exact day a flight took place, and wait_min contains the amount of minutes it took travelers to wait at the gate. To make your analysis easier, you want to create two new categorical variables:\n","\n","wait_type: 'short' for 0-60 min, 'medium' for 60-180 and long for 180+\n","day_week: 'weekday' if day is in the weekday, 'weekend' if day is in the weekend.\n","The pandas and numpy packages have been imported as pd and np. Let's create some new categorical data!\n","\n","Create the ranges and labels for the wait_type column mentioned in the description.\n","\n","Create the wait_type column by from wait_min by using pd.cut(), while inputting label_ranges and label_names in the correct arguments.\n","\n","Create the mapping dictionary mapping weekdays to 'weekday' and weekend days to 'weekend'.\n","\n","Create the day_week column by using .replace()."],"metadata":{"id":"5Qj5yhc5GUtG"}},{"cell_type":"code","source":["# Create ranges for categories\n","label_ranges = [0, 60, 180, np.inf]\n","label_names = ['short', 'medium', 'long']\n","\n","# Create wait_type column\n","airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges,\n","                               labels = label_names)\n","\n","# Create mappings and replace\n","mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday',\n","            'Thursday': 'weekday', 'Friday': 'weekday',\n","            'Saturday': 'weekend', 'Sunday': 'weekend'}\n","\n","airlines['day_week'] = airlines['day'].replace(mappings)"],"metadata":{"id":"zc7nGlV3GTJY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cleaning Text Data"],"metadata":{"id":"T8uy3eN5GsyK"}},{"cell_type":"code","source":["# Replace \"Dr.\" with empty string \"\"\n","airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n","\n","# Replace \"Mr.\" with empty string \"\"\n","airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\",\"\")\n","\n","# Replace \"Miss\" with empty string \"\"\n","airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\",\"\")\n","\n","# Replace \"Ms.\" with empty string \"\"\n","airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\",\"\")\n","\n","# Assert that full_name has no honorifics\n","assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False\n","\n","# Store length of each row in survey_response column\n","resp_length = airlines['survey_response'].str.len()\n","\n","# Find rows in airlines where resp_length > 40\n","airlines_survey = airlines[resp_length > 40]\n","\n","# Assert minimum survey_response length is > 40\n","assert airlines_survey['survey_response'].str.len().min() > 40\n","\n","# Print new survey_response column\n","print(airlines_survey['survey_response'])"],"metadata":{"id":"RKh28dSDGvN_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Uniformity"],"metadata":{"id":"UJIObga3IwS-"}},{"cell_type":"markdown","source":["Find the rows of acct_cur in banking that are equal to 'euro' and store them in the variable acct_eu.\n","Find all the rows of acct_amount in banking that fit the acct_eu condition, and convert them to USD by multiplying them with 1.1.\n","Find all the rows of acct_cur in banking that fit the acct_eu condition, set them to 'dollar'."],"metadata":{"id":"kNqV-kQgXrp2"}},{"cell_type":"code","source":["# Find values of acct_cur that are equal to 'euro'\n","acct_eu = banking['acct_cur'] == 'euro'\n","\n","# Convert acct_amount where it is in euro to dollars\n","banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n","\n","# Unify acct_cur column by changing 'euro' values to 'dollar'\n","banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n","\n","# Assert that only dollar currency remains\n","assert banking['acct_cur'].unique() == 'dollar'\n","\n"],"metadata":{"id":"DBwCJNyWIyiQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convert the account_opened column to datetime, while making sure the date format is inferred and that erroneous formats that raise error return a missing value."],"metadata":{"id":"APh5MwLuYxx0"}},{"cell_type":"code","source":["# Print the header of account_opened\n","print(banking['account_opened'].head())\n","\n","# Convert account_opened to datetime\n","banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n","                                           # Infer datetime format\n","                                           infer_datetime_format = True,\n","                                           # Return missing value for error\n","                                           errors = 'coerce')"],"metadata":{"id":"YXWwEAzjYuL7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Extract the year from the amended account_opened column and assign it to the acct_year column.\n","\n","Print the newly created acct_year column."],"metadata":{"id":"8gyLUDo3Y1hr"}},{"cell_type":"code","source":["# Get year of account opened\n","banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n","\n","# Print acct_year\n","print(banking['acct_year'])"],"metadata":{"id":"C8u8BdwPY2o1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cross Field Validation"],"metadata":{"id":"_eUA0_AEZUdf"}},{"cell_type":"code","source":["# Store fund columns to sum against\n","fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n","\n","# Find rows where fund_columns row sum == inv_amount\n","inv_equ = banking[fund_columns].sum(axis = 1) == banking['inv_amount']\n","\n","# Store consistent and inconsistent data\n","consistent_inv = banking[inv_equ]\n","inconsistent_inv = banking[~inv_equ]\n","\n","# Store consistent and inconsistent data\n","print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])\n"],"metadata":{"id":"5u5Oqcskmpfu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Store today's date into today, and manually calculate customers' ages and store them in ages_manual.\n","Find all rows of banking where the age column is equal to ages_manual and then filter banking into consistent_ages and inconsistent_ages."],"metadata":{"id":"Bu7phwigookY"}},{"cell_type":"code","source":["# Store today's date and find ages\n","today = dt.date.today()\n","ages_manual = today.year - banking['birth_date'].dt.year\n","\n","# Find rows where age column == ages_manual\n","age_equ = banking['age'] == ages_manual\n","\n","# Store consistent and inconsistent data\n","consistent_ages = banking[age_equ]\n","inconsistent_ages = banking[~age_equ]\n","\n","# Store consistent and inconsistent data\n","print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"],"metadata":{"id":"HTI_vtClona6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Completeness"],"metadata":{"id":"gstWN6XEouvs"}},{"cell_type":"markdown","source":["Print the number of missing values by column in the banking DataFrame.\n","Plot and show the missingness matrix of banking with the msno.matrix() function."],"metadata":{"id":"MG3NUtrtqdEW"}},{"cell_type":"code","source":["# Print number of missing values in banking\n","print(banking.isna().sum())\n","\n","# Visualize missingness matrix\n","msno.matrix(banking)\n","plt.show()\n"],"metadata":{"id":"eLPEvoygow0K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Isolate the values of banking missing values of inv_amount into missing_investors and with non-missing inv_amount values into investors."],"metadata":{"id":"xFG7OElCrtGj"}},{"cell_type":"code","source":["# Isolate missing and non missing values of inv_amount\n","missing_investors = banking[banking['inv_amount'].isna()]\n","investors = banking[~banking['inv_amount'].isna()]"],"metadata":{"id":"0pP6JLcjrryx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print number of missing values in banking\n","print(banking.isna().sum())\n","\n","# Visualize missingness matrix\n","msno.matrix(banking)\n","plt.show()\n","\n","# Isolate missing and non missing values of inv_amount\n","missing_investors = banking[banking['inv_amount'].isna()]\n","investors = banking[~banking['inv_amount'].isna()]\n","\n","# Sort banking by age and visualize\n","banking_sorted = banking.sort_values(by = 'age')\n","msno.matrix(banking_sorted)\n","plt.show()\n","\n","# Drop missing values of cust_id\n","banking_fullid = banking.dropna(subset = ['cust_id'])\n","\n","# Compute estimated acct_amount\n","acct_imp = banking_fullid['inv_amount'] * 5\n","\n","# Impute missing acct_amount with corresponding acct_imp\n","banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n","\n","# Print number of missing values\n","print(banking_imputed.isna().sum())"],"metadata":{"id":"qpr3MGrlsWrm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Comparing Strings"],"metadata":{"id":"DxbNJkNwt_tW"}},{"cell_type":"code","source":["# Import process from thefuzz\n","from thefuzz import process\n","\n","# Store the unique values of cuisine_type in unique_types\n","unique_types = restaurants['cuisine_type'].unique()\n","\n","# Calculate similarity of 'asian' to all values of unique_types\n","print(process.extract('asian', unique_types, limit = len(unique_types)))\n","\n","# Calculate similarity of 'american' to all values of unique_types\n","print(process.extract('american', unique_types, limit = len(unique_types)))\n","\n","# Calculate similarity of 'italian' to all values of unique_types\n","print(process.extract('italian', unique_types, limit = len(unique_types)))\n","-------------------------------------------------------------------------------\n","# Inspect the unique values of the cuisine_type column\n","print(restaurants['cuisine_type'].unique())\n","\n","# Create a list of matches, comparing 'italian' with the cuisine_type column\n","matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n","\n","# Inspect the first 5 matches\n","print(matches[0:5])\n","\n","# Iterate through the list of matches to italian\n","for match in matches:\n","  # Check whether the similarity score is greater than or equal to 80\n","  if match[1] >= 80:\n","    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n","    restaurants.loc[restaurants['cuisine_type']== match[0], 'cuisine_type'] = 'italian'\n","\n"," # Iterate through the list of matches\n","  for match in matches:\n","     # Check whether the similarity score is greater than or equal to 80\n","    if match[1] >= 80:\n","      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n","      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n","\n","# Inspect the final result\n","print(restaurants['cuisine_type'].unique())"],"metadata":{"id":"qMS_yI1ZuCZj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generating Pairs"],"metadata":{"id":"aFdsgYs1Luy-"}},{"cell_type":"code","source":["# Create an indexer and object and find possible pairs\n","indexer = recordlinkage.Index()\n","\n","# Block pairing on cuisine_type\n","indexer.block('cuisine_type')\n","\n","# Generate pairs\n","pairs = indexer.index(restaurants, restaurants_new)\n","\n","# Create a comparison object\n","comp_cl = recordlinkage.Compare()\n","\n","# Find exact matches on city, cuisine_types\n","comp_cl.exact('city', 'city', label='city')\n","comp_cl.exact('cuisine_type', 'cuisine_type', label = 'cuisine_type')\n","\n","# Find similar matches of rest_name\n","comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8)\n","\n","# Get potential matches and print\n","potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n","print(potential_matches)\n","\n"],"metadata":{"id":"0cABQOtQLwlW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Linking DataFrames"],"metadata":{"id":"AgLJ8a6zQ5qX"}},{"cell_type":"code","source":["# Isolate potential matches with row sum >=3\n","matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n","\n","# Get values of second column index of matches\n","matching_indices = matches.index.get_level_values(1)\n","\n","# Subset restaurants_new based on non-duplicate values\n","non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n","\n","# Append non_dup to restaurants\n","full_restaurants = restaurants.append(non_dup)\n","print(full_restaurants)"],"metadata":{"id":"sq7Hpeh-Q8F7"},"execution_count":null,"outputs":[]}]}